{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a probabilistic classifier that gets its name because is based on:<br>\n",
    "* **Naive assumption** that the predictive features are mutually independent. This assumption simplifies probability calculation.\n",
    "* **Bayes' theorem**, that computes the probabilities of each predictive attribute of the data belonging to each class, in order to make a prediction of probability distribution over all classes.\n",
    "<br>\n",
    "\n",
    "Even though the assumption that the features are uncorrelated is rarely to happen in real-life, this algorithm still returns a good accuracy in practice.<br>\n",
    "Also, NB classifier usually is quite robust to irrelevant features, is fast and does not require a lot of memory. That's why it is traditionally the algorithm of choice for real-world apps that require to respond user's requests instantaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the Bayes' theorem, we first have to start by understanding conditional probability. It is a measure of the probability of an event occuring, A, given that another event has already occurred , B(evidence). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and viceversa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes' theorem provides a formula to calculate the probability of an event, given the occurrence of some other related events. Used to update a previous belief once new information is obtained.\n",
    "<br> <br>\n",
    "It will be useful for us as a way of going from P(X|y), known from the training set, to find P(y|X) from the test set.\n",
    "So, replacing the A and B in the above conditional formula, we can say that Bayes Rule is a way to go from:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(X|y) = \\frac{P(X \\cap y)}{P(y)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(y|X) = \\frac{P(y \\cap X)}{P(X)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the Bayes' Rule will be a way to predict the P(Outcome|Evidence) in the test set, using the information obtained in the training set: P(Evidence|Outcome). <br><br>\n",
    "The Bayes' Rule calculates P(y|X), which is the probability that y occurs given that X is true:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(y|X) = \\frac{P(X|y) P(y)}{P(X)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be understood as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Posterior = \\frac{Likelihood * Prior}{Evidence}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:<br>\n",
    "* **Prior == P(y)**: What is originally belief befor new evidence is introduced.<br>\n",
    "* **Posterior == ùëÉ(ùë¶|ùëã)**: Probability for y knowing the values of each feature X. Posterior probability distributions should be a better reflection of the underlying truth of a data generating process than the prior probability since it is including more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a typical probability example with coins to understand it better:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We have two coins, one is unfair with 80% of heads getting a head and 20% tail. The other coin is fair. We pick one of the coins randomly and flip it. What is the probability that this is the unfair coin if we got a head?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Notation: <br> \n",
    "    * U: Event of picking the unfair coin.\n",
    "    * F: Event of picking the fair coin.\n",
    "    * H: Event of getting a head.\n",
    "   \n",
    "    And we have that the probability that we picked the unfair coin would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(U|H) = \\frac{P(H|U) P(U)}{P(H)} = \\frac{0.8 * 0.5}{0.8 * 0.5 + 0.5 * 0.5} = 0.62\n",
    "\\end{equation*}\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bayes' theorem to Classification: Being Naive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how we take advantage of the occurrance of an event to update our prior knowledge. But, in ML classification problems we will have more than one feature. <br><br>\n",
    "In general, we can state that given a set of features $X_1$, ..., $X_n$, we will want to calculate the (posterior) probability for each of the possible different classes ($y_1$, ..., $y_k$).<br>\n",
    "This means that we will not only classify each observation into one specific class, but we will also want to know what is the probability for that observation to be catalogued in each of the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bayes' theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(y_j|X_1,..., X_n) = \\frac{P(X_1, ..., X_n|y_j) P(y_j)}{P(X_1, ..., X_n)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **calculation of the prior, P($y_j$)**, is **easy**. We can estimate it by dividing the frequency of observations in the training data that have that class label by the total number of observations in the training set. <br><br>\n",
    "* The **calculation of the conditional probability**, likelihood, is **not feasable** unless we have so many observations that we can effectively estimate the probability distribution for all different possible combination of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*->Now is the moment when **being Naive** will help us!.* If we naively assume that $X_1$, ..., $X_n$ are independent from each other, we simplify the calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(X_1,..., X_n|y_j) = P(X_1|y_j) ... P(X_n|y_j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as the denominator, $P(X_1, ..., X_n)$,  is a constant that will be the same for all the classes, we can remove it too.<br><br>\n",
    "So, the **final formula when using Naive Bayes:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(y_j|X_1,..., X_n) = P(y_j) P(X_1|y_j) ... P(X_n|y_j)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation will be performed for each of the class labels. The label with the largest probability will be selected as the classification for a given instance (MAP - Maximum a posteriori decision rule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing about Naive Bayes is that although it is not theoretically correct to simply assume independance when we feel like it, in this case, it has been proved to work really well in real-world apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace/ Lidstone smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think carefully about the formula above, we'll see one possible issue: What happens if a categorical variable, i.e. $X_2$, in the test set takes a value that was not present in the training set?.<br>\n",
    "In that case $P(X_2|y_j)$ = 0, and then the probability for the entire class will be 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "P(y_j|X_1,..., X_n) = P(y_j) P(X_1|y_j) * 0 * P(X_3|y_j) ... P(X_n|y_j) = 0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this issues with unseen values for features, we can use a smoothing technique: **Laplace estimation**. This is a very simple technique taht adds one to all distinct possible categories/feature occurrences. <br>\n",
    "The intuition behind this technique is that even though we have not seen a given category, i.e. a word, in the training set, there is still a chance that it will be present in the whole set. So we are pretending that we have seen every occurrence once more that we actually did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *If we do not smooth by 1 but by an adjustable parameter, alpha>0, it is called Lidstone smoothing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, a different approach is required depending on the data type of the features:\n",
    "* **Binary**: Binomial distribution -> **Bernoulli Naive Bayes**\n",
    "* **Categorical**: Multinomial distribution -> **Multinomial Naive Bayes**\n",
    "* **Numerical continuous**: Gaussian distribution -> **Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* *If we have a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n",
    "Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian classifiers are widely used in medical diagnosis, weather forecasts, sentiment analysis, and for quick classification of documents (e.g., spam/nonspam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when dataset is huge\n",
    "When you have small training set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
